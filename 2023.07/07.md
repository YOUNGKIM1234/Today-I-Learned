# Today-I-Learned

지난 기간동안 한 일 : \
    1. spark에서 bottom부터 n개의 데이터 자르기 \
        - 자른 후 Object가 아니라 Dataset으로 리턴해야해서 tail 함수 사용 불가 \
        - reverse ordering 역시 이후 데이터를 순서대로 탐색해야하기 때문에 사용 불가 \
        - JavaRDD로 만들어서 zipWithIndex 사용함. 그럼 JavaPairRDD가 되고, 따로 붙은 Index를 컬럼으로 포함시켜야 JavaRDD로 변환할 수 있는데, 이 때 index 검사도 같이 해서 Row를 null로 만든 후 null을 filtering.



## TIL
    1. zipWithIndex와 monotonically_increasing_id의 차이, 그와 비슷한 spark Window 함수
    2. index를 만들 때 monotonically_increasing_id를 쓰면 안되는 이유 : 파티션마다 index를 붙이는거라 데이터가 커지면 일관된 index를 얻을 수 없음. 반드시 한 파티션으로 모아야하는데 그러면 OOM위험 있음.
    3. JavaPairRDD를 JavaRDD로 바꾸는 방법
        (참고 https://stackoverflow.com/questions/41302666/converting-javardd-to-dataframe-in-spark-java)
    4. StructType을 만들어 JavaRDD를 DataFrame으로 바꾸는 방법
        (참고 https://stackoverflow.com/questions/58634644/how-to-create-complex-structtype-schema-in-spark-java)